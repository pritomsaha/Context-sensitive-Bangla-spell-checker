# Context-sensitive-Bangla-spell-checker
Software Project Lab 3

## Proposed context-sensitive spell checking approach and implementation
In Spell Checker, two major steps are misspelling detection and then providing suggestions of most likely valid words. The proposed spell-checking approach can detect non-word and real-word errors in the text and provide suggestions. There are challenges for detecting both of the errors mentioned above. For detecting, if a word is a non-word error, it is just checked if the word is in the lexicon. For this project, we have generated a lexicon with 112802 Bangla words. If a word is not in the
lexicon, then the stemmed version may be in the lexicon. For
this, a rule-based Bangla stemmer has been used to get the stem of a Bangla word which takes a file with some defined rules for stemming a given word. For example, for the word "কাজটি", the stemmed version of this word will be "কাজ". If the stemmed word also is not in the
lexicon, then the word is marked as a non-word error, and for this, the valid suggestions should be generated.

In the real-word, there are contextual errors that depend on the word's context. To find if a word is contextually inappropriate in a sentence, we can not think about the word in an isolated way; the words surrounding the word should be in the calculation to detect a real-word error. Here word vectors generated before using word2vec models will be used. First, a confusion set or candidate words set is generated, and how
this set is produced that will be explained in detail in the next section of the suggestion generation.
At first, cosine similarity of context words with the given word and all the confusion set members are calculated. Cosine similarity is calculated by taking the dot product of the two word vectors' unit-vector. Before calculating the dot product, vectors of context
words are averaged into one single vector. Here for context words, we have chosen the left two words and right two words of the given word. After calculating cosine similarity, we will get a similarity score between -1 and 1. But here, if a cosine similarity value is less than 0, we have turned them to 0(zero). It may happen that the given word
to be detected or any word in the confusion set may not be in the vocabulary set of
word vectors, for this we produced the stem of this word and if stemmed word is not
also in the vocabulary of word vectors then we set the cosine similarity for that word
as 0(zero). Same problem of not existing in the vocabulary set of word vectors may
happen for context words. For this, we use the same technique of stemming the word
and get the vector of stemmed word and calculate the average o context word vectors.
If the stemmed word also does not have the corresponding vector then we ignore the
word and move to the same direction for another word and same technique for this
word. If there are no context words in the left side or we don't get any context word
vector, then we ignore the given word for detection for real-word error and go to the
next word.
After performing the above calculation now we have cosine similarity value of all
words in the confusion set and the word to be detected with the context words. Now
the question how we use these cosine similarity value for detecting the given word as
if it is contextually correct or inappropriate. For this we first find the max similarity
value within words in confusion set and use a threshold value to compare this max
value with similarity score of given word to be detected. Here the threshold value
is used 0.1 as experiment and multiply this threshold value with the similarity score
of the given word and compare the multiplied result with the max value of cosine
similarity with in confusion set. If the the result is less than max similarity score
then, we marked the given word as real-word error and for this valid most likely
suggestion should be generated.

After detecting a word as misspelled, suggestion of most likely valid words is gen-
erated. Here for both non-word and real-word error, same suggestion generation
technique has been used. Here words in confusion set are generated from words which are phonetically similar
or are generated by one edit distance. Here double metaphone phonetic encoding
technique has been used to encode word so that phonetically related words would
have same or nearly distanced encoding. For example, after double metaphone encoding
of অনয্ and অন্ will have same encoding 'onn'. The encoded word and the original
Bangla word of every dictionary term are associated and stored in a datastructure
in the pre-calculation step. After this for every encoded word there will be a list of
original Bangla words mapped with the encoded word.
If we use conventional possible terms generation up-to 1 edit distance of input word
by deleting, transporting, replacing and inserting then for edit distance 1 and input
word length of l it needs l deletions, l − 1 transposition, l + 1 insertion, l alterations.
If we want to pre-calculate possible terms for 1 edit distance for all dictionary terms
it is computationally expensive. To improve the speed of possible terms generation
of certain edit distance, we have used the technique of FAROO's Symmetric Delete
Spelling Correction
In the pre-calculation term all terms with upto one edit distance (deletes only) of
every phonetically encoded words in the dictionary have been produced, then these
delete words have been associated with their original encoded term and stored in
a datastructure. For example for a delete encoded delete word 'on' the associated
original encoded words are' onn',' ont',' osn',' oln',' onD',' onn',' kon',' onu',' okn',
' ojn',' ond',' ojn',' onl',' omn',' ont',' oyn',' onr'. From previous Bangla word and
encoded word association we can get the all the original Bangla words from all of these 
encoded word. Now for any input word, following steps are followed for producing
confusion set:
First the encoded word of the input word is produced using same double meta-
phone algorithm.
An empty confusion set is initialized.
The encoded input word is then checked if it is in the previous stored original
Bangla word and encoded word association, if there is an entry with this encoded
input word then the associated all Bangla words with this encoded word are
added to the confusion set.
Then all the words with one edit distance using only delete operations are
obtained and for each of these words, it is checked if it exists in the previously
stored delete word and encoded word association and if it exists then all the
mapped encoded words are retrieved.
At last from all the retrieved encoded words original Bangla words are obtained
and added to confusion set.

After generating the confusion set we have to set some criteria to sort these words so
that most likely words should be in the top of the suggestion list. Typographic edit distance, Phonetic edit distance and Cosine similarity these three 
parameters are used for ranking the suggestion.

## Corpus generation
A corpus or text corpus is a large and structured set of texts. As in the proposed
system, real word error will be detect which depends on the context knowledge of the
word based on surrounding words in the text. For knowing the context of word, simi-
larity or appropriateness of a word in a sentence here word2vec is use for representing
Bangla words in vector form so that using some mathematical vector operation we
can find out the proximity of two or more words. For creating vector representation of
word we need a large corpus of text. So for training a model for Bangla language, large
set of text is collected to make a corpus of Bangla language. From different sources
like wikipedia, Bangla newspaper, Bangla literature collected Bangla texts are col-
lected . A program was writen for crawling text from website to collect Bangla text
of literature from website ebanglalibrary . For other news crawl and wikipedia data
available data for Bangla from website Leipzig Corpora Collection are collected. Here
a python library Requests is used to get the source of a webpage and python parser
library Beautiful Soup (BS4) to parse html and xml documents. After collecting text,
those were splited into sentences.

## Training corpus using word2vec
As mentioned before, for learning word vectors, word2vec is used to train the gener-
ated corpus of Bangla text. Word vector is simply a weighted vector of a specified
dimension where every element in the vector represents weight. In word2vec, an un-
labeled training corpus is given, by learning from this corpus, a vector for each unique
in the corpus is produced in such way that vector can encode the semantic informa-
tion of a word. From these vector we can easily measure the similarity between two
words by calculating the cosine similarity of the corresponding word vectors of those
words. Words with nearly closed semantic meanings will have similarity vectors in
terms cosine similarity. 

Here Gensim python library gensim.models.word2vec function
to train the model. This function takes list of sentence as input where sentence is a list
of words. Before training there some preprocessing(cleaning) have been done on every
sentence in the corpus. First every character except Bangla alphabets (only alphabet
not numerical) are removed from sentence. Then Bangla stop-words(commonly used
word) have been removed from sentence. Here as hidden layer size 300 has been which
gives comparatively good result and 5 window size has been used in the training which
means in the time of training in input layer five left and five right context word will be used to learn the model. Here several model has been trained using both softmax
function and negative sampling. The specified function will return a model which can
be used to retrain with more sentences at any time.

## How to Use
Prerequisites: Git, Python-3 Django 1.11.2 with virtual environment.
Following are the steps for running the project. 
##### a) go to Bangla-spell_checker directory and activate virtual environment
    source bin/activate
##### b) go to Bangla-spell_checker/src directory and run the project 
    python manage.py runserver [ip:port](default is localhost:8000)
The project will be run in localhost and type the ip and port in the browser and continue

## Helpful Links
* For tutorial about word2vec model architecture use this link: http://mccormickml.com/2016/04/27/word2vec-resources/
